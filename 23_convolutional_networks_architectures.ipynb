{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e181825e",
   "metadata": {},
   "source": [
    "# Convolutional Networks Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "422c7ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision as tv\n",
    "from torchsummary import summary\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81bf257e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90b9a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, device):\n",
    "    acc_sum, n = 0, 0\n",
    "    net.eval()\n",
    "    for X, y in data_iter:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        acc_sum +=(net(X).argmax(axis=1) == y).sum()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum.item() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2e7e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, optimizer, num_epochs, device):\n",
    "    loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Step {i}, time since epoch: {time.time() - start:.3f}. \"\n",
    "                      f\"Train acc: {train_acc_sum / n:.3f}. Train Loss: {train_l_sum / n:.3f}\")\n",
    "        test_acc = evaluate_accuracy(test_iter, net.to(device), device)\n",
    "        print(f\"epoch {epoch + 1}, loss {train_l_sum / n:.4f}, train acc {train_acc_sum / n:.3f}\" \\\n",
    "              f\", test acc {test_acc:.3f}, time {time.time() - start:.1f} sec\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1268d13",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4f70cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "transforms = tv.transforms.Compose([\n",
    "    tv.transforms.Resize((224, 224)),\n",
    "    tv.transforms.ToTensor()\n",
    "])\n",
    "train_dataset = tv.datasets.MNIST('./datas', train=True, transform=transforms, download=True)\n",
    "test_dataset = tv.datasets.MNIST('./datas', train=False, transform=transforms, download=True)\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a842a8",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87df0104",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 96, kernel_size=11, stride=4),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(6400, 4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(4096, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8081855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 54, 54]          11,712\n",
      "              ReLU-2           [-1, 96, 54, 54]               0\n",
      "         MaxPool2d-3           [-1, 96, 26, 26]               0\n",
      "            Conv2d-4          [-1, 256, 26, 26]         614,656\n",
      "              ReLU-5          [-1, 256, 26, 26]               0\n",
      "         MaxPool2d-6          [-1, 256, 12, 12]               0\n",
      "            Conv2d-7          [-1, 384, 12, 12]         885,120\n",
      "              ReLU-8          [-1, 384, 12, 12]               0\n",
      "            Conv2d-9          [-1, 384, 12, 12]       1,327,488\n",
      "             ReLU-10          [-1, 384, 12, 12]               0\n",
      "           Conv2d-11          [-1, 256, 12, 12]         884,992\n",
      "             ReLU-12          [-1, 256, 12, 12]               0\n",
      "        MaxPool2d-13            [-1, 256, 5, 5]               0\n",
      "          Flatten-14                 [-1, 6400]               0\n",
      "           Linear-15                 [-1, 4096]      26,218,496\n",
      "             ReLU-16                 [-1, 4096]               0\n",
      "          Dropout-17                 [-1, 4096]               0\n",
      "           Linear-18                 [-1, 4096]      16,781,312\n",
      "             ReLU-19                 [-1, 4096]               0\n",
      "          Dropout-20                 [-1, 4096]               0\n",
      "           Linear-21                   [-1, 10]          40,970\n",
      "================================================================\n",
      "Total params: 46,764,746\n",
      "Trainable params: 46,764,746\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 10.22\n",
      "Params size (MB): 178.39\n",
      "Estimated Total Size (MB): 188.81\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(net.to(device), input_size=(1, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc745866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, time since epoch: 11.125. Train acc: 0.082. Train Loss: 2.303\n",
      "Step 10, time since epoch: 122.948. Train acc: 0.108. Train Loss: 2.337\n",
      "Step 20, time since epoch: 230.337. Train acc: 0.212. Train Loss: 2.240\n",
      "Step 30, time since epoch: 337.195. Train acc: 0.268. Train Loss: 2.080\n",
      "Step 40, time since epoch: 444.090. Train acc: 0.375. Train Loss: 1.812\n",
      "Step 50, time since epoch: 551.769. Train acc: 0.454. Train Loss: 1.590\n",
      "Step 60, time since epoch: 658.362. Train acc: 0.515. Train Loss: 1.418\n",
      "Step 70, time since epoch: 766.390. Train acc: 0.564. Train Loss: 1.279\n",
      "Step 80, time since epoch: 873.164. Train acc: 0.606. Train Loss: 1.158\n",
      "Step 90, time since epoch: 980.556. Train acc: 0.641. Train Loss: 1.058\n",
      "Step 100, time since epoch: 1088.046. Train acc: 0.670. Train Loss: 0.975\n",
      "Step 110, time since epoch: 1196.009. Train acc: 0.695. Train Loss: 0.903\n",
      "Step 120, time since epoch: 1303.899. Train acc: 0.715. Train Loss: 0.845\n",
      "Step 130, time since epoch: 1412.531. Train acc: 0.733. Train Loss: 0.794\n",
      "Step 140, time since epoch: 1520.244. Train acc: 0.749. Train Loss: 0.747\n",
      "Step 150, time since epoch: 1628.076. Train acc: 0.763. Train Loss: 0.705\n",
      "Step 160, time since epoch: 1736.131. Train acc: 0.775. Train Loss: 0.669\n",
      "Step 170, time since epoch: 1843.269. Train acc: 0.786. Train Loss: 0.637\n",
      "Step 180, time since epoch: 1950.511. Train acc: 0.796. Train Loss: 0.608\n",
      "Step 190, time since epoch: 2057.218. Train acc: 0.805. Train Loss: 0.583\n",
      "Step 200, time since epoch: 2163.626. Train acc: 0.812. Train Loss: 0.561\n",
      "Step 210, time since epoch: 2269.474. Train acc: 0.820. Train Loss: 0.540\n",
      "Step 220, time since epoch: 2374.243. Train acc: 0.827. Train Loss: 0.520\n",
      "Step 230, time since epoch: 2479.512. Train acc: 0.833. Train Loss: 0.500\n",
      "epoch 1, loss 0.4942, train acc 0.835, test acc 0.983, time 2646.9 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 1\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "train(net, train_iter, test_iter, optimizer, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d368d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_kernel",
   "language": "python",
   "name": "pytorch_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
